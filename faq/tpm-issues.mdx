---
title: Tokens-per-minute rate limiting
sidebarTitle: TPM rate limiting
---

![Chat with pdf](/images/tpm_issues.png)

If you encounter any issues with proprietary models (such as OpenAI models) due to rate limiting, we provide options to configure `retries`, enable `exponential_backoff=True`, and set `delay_between_retries` to a specific value in seconds (default is 1 second).

Please note that this retry mechanism does not work in stream mode. For instance, using `agent.print_response` with `stream=True` will bypass this retry mechanism.

For example:
```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You are an enthusiastic news reporter with a flair for storytelling!",
    markdown=True,
    retries=10,
    exponential_backoff=True,
    delay_between_retries=2
)
agent.print_response("Tell me about a breaking news story from New York.", stream=True)
```

See our [models documentation](../models/) for specific information about rate limiting.

In the case of OpenAI, they have tier based rate limits. See the [docs](https://platform.openai.com/docs/guides/rate-limits/usage-tiers) for more information.
